{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XwqhoAEbhebX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noor-Z1/CNG-491-492/blob/main/tests/Imputation_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processing\n"
      ],
      "metadata": {
        "id": "XwqhoAEbhebX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfjmtuwD3xBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3bdbf5-351e-47bc-8481-2b211fa36dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# this cell is for mounting the drive to access the data\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# you can access the extracted dataset from here:\n",
        "# https://drive.google.com/drive/folders/19juiVcKYgml1CUT0hJgWYWkHwghX2O5i?usp=sharing\n",
        "\n",
        "# after having the dataset in your drive, mount google drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBnu9KqnhnhU",
        "outputId": "16efc6a7-4c6f-47e9-9402-ee86d7c90178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.3.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "-ioF-VuKhp0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is for preprocessing the sensors_data.txt\n",
        "# to understand the columns of sensors data check: column_names.txt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_sensors_data(file_path):\n",
        "    start_column = 1\n",
        "    end_column = 250\n",
        "\n",
        "    # Generate column names as a range of numbers\n",
        "    column_names = list(range(start_column, end_column + 1))\n",
        "    df = pd.read_csv(file_path, sep=' ')\n",
        "    df.columns = column_names\n",
        "    return df"
      ],
      "metadata": {
        "id": "KdW5U964hqi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set -> person 2 and 3\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# we choose 1 to 250 as the column_names.txt has defined the columns in this way\n",
        "start_column = 1\n",
        "end_column = 250\n",
        "\n",
        "# Generate column names as a range of numbers\n",
        "column_names = list(range(start_column, end_column + 1))\n",
        "\n",
        "# Create an empty DataFrame with specified column names\n",
        "df_train = pd.DataFrame(columns=column_names)\n",
        "\n",
        "# change the file path accordingly\n",
        "for i in range(2,4):\n",
        "  for j in range(1,6):\n",
        "    file_path = '/content/gdrive/MyDrive/Opportunity_extracted/data/S{}-ADL{}/S{}-ADL{}_sensors_data.txt'.format(i, j, i, j)\n",
        "    print(file_path)\n",
        "    df = preprocess_sensors_data(file_path)\n",
        "    df_train = pd.concat([df_train, df],ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972Gpsnrhv0B",
        "outputId": "c0c887f5-815d-4d0f-df64-37ba93a9fad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S2-ADL1/S2-ADL1_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S2-ADL2/S2-ADL2_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S2-ADL3/S2-ADL3_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S2-ADL4/S2-ADL4_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S2-ADL5/S2-ADL5_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S3-ADL1/S3-ADL1_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S3-ADL2/S3-ADL2_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S3-ADL3/S3-ADL3_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S3-ADL4/S3-ADL4_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S3-ADL5/S3-ADL5_sensors_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set -> person 4\n",
        "\n",
        "# we choose 1 to 250 as the column_names.txt has defined the columns in this way\n",
        "start_column = 1\n",
        "end_column = 250\n",
        "\n",
        "# Generate column names as a range of numbers\n",
        "column_names = list(range(start_column, end_column + 1))\n",
        "\n",
        "# Create an empty DataFrame with specified column names\n",
        "df_test = pd.DataFrame(columns=column_names)\n",
        "\n",
        "# change the file path accordingly\n",
        "for i in range(4,5):\n",
        "  for j in range(1,6):\n",
        "    file_path = '/content/gdrive/MyDrive/Opportunity_extracted/data/S{}-ADL{}/S{}-ADL{}_sensors_data.txt'.format(i, j, i, j)\n",
        "    print(file_path)\n",
        "    df = preprocess_sensors_data(file_path)\n",
        "    df_test = pd.concat([df_test, df],ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_N-SHnYh-Fr",
        "outputId": "93b13994-0c10-4247-bf3f-f2952d2e8a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S4-ADL1/S4-ADL1_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S4-ADL2/S4-ADL2_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S4-ADL3/S4-ADL3_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S4-ADL4/S4-ADL4_sensors_data.txt\n",
            "/content/gdrive/MyDrive/Opportunity_extracted/data/S4-ADL5/S4-ADL5_sensors_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the columns needed only\n",
        "needed = [26,10,8,41,9,11,25,131,101,39, 128, 24, 98, 38, 15, 134]\n",
        "df_test = df_test[df_test.columns[needed]]\n",
        "print(df_test)\n",
        "df_train = df_train[df_train.columns[needed]]\n",
        "print(df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fURqR2UOiQ2Y",
        "outputId": "db0a7c1b-4b09-456b-b1c9-7798720a271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           27     11     9     42     10      12     26    132    102    40   \\\n",
            "0       1076.0  262.0  861.0  21.0  367.0   943.0   93.0  -3.0  144.0  141.0   \n",
            "1       1064.0  234.0  839.0   5.0  367.0   960.0  102.0  -6.0  144.0  143.0   \n",
            "2       1060.0  262.0  873.0  13.0  361.0   944.0   92.0  10.0  146.0  151.0   \n",
            "3       1087.0  223.0  833.0   6.0  389.0   995.0   79.0  45.0  152.0  131.0   \n",
            "4       1119.0  157.0  809.0  73.0  504.0  1084.0  -28.0  18.0  165.0  136.0   \n",
            "...        ...    ...    ...   ...    ...     ...    ...   ...    ...    ...   \n",
            "147204     NaN    NaN    NaN   NaN    NaN     NaN    NaN  53.0    NaN    NaN   \n",
            "147205     NaN    NaN    NaN   NaN    NaN     NaN    NaN  -2.0    NaN    NaN   \n",
            "147206     NaN    NaN    NaN   NaN    NaN     NaN    NaN   5.0    NaN    NaN   \n",
            "147207     NaN    NaN    NaN   NaN    NaN     NaN    NaN  51.0    NaN    NaN   \n",
            "147208     NaN    NaN    NaN   NaN    NaN     NaN    NaN   0.0    NaN    NaN   \n",
            "\n",
            "         129    25     99     39     16     135  \n",
            "0      -35.0  753.0 -656.0 -114.0  210.0  185.0  \n",
            "1      -13.0  691.0 -655.0 -116.0  205.0  183.0  \n",
            "2       23.0  767.0 -655.0 -120.0  215.0  194.0  \n",
            "3      -13.0  906.0 -655.0  -96.0  200.0  177.0  \n",
            "4       -8.0  973.0 -657.0 -125.0  116.0  191.0  \n",
            "...      ...    ...    ...    ...    ...    ...  \n",
            "147204  22.0    NaN    NaN    NaN    NaN    NaN  \n",
            "147205  -6.0    NaN    NaN    NaN    NaN    NaN  \n",
            "147206   1.0    NaN    NaN    NaN    NaN    NaN  \n",
            "147207  26.0    NaN    NaN    NaN    NaN    NaN  \n",
            "147208   0.0    NaN    NaN    NaN    NaN    NaN  \n",
            "\n",
            "[147209 rows x 16 columns]\n",
            "          27     11     9     42     10     12     26   132   102   40   129  \\\n",
            "0       520.0  706.0  392.0 -35.0  783.0 -689.0  943.0    6  -7.0 -26.0  -16   \n",
            "1       522.0  678.0  393.0 -60.0  778.0 -673.0  913.0  -39 -11.0 -17.0    8   \n",
            "2       550.0  688.0  387.0 -21.0  755.0 -657.0  908.0    6 -24.0 -19.0   -1   \n",
            "3       535.0  692.0  389.0  -5.0  770.0 -702.0  914.0   51 -44.0  -7.0  -19   \n",
            "4       534.0  688.0  398.0  12.0  747.0 -695.0  917.0   22 -67.0   1.0  -29   \n",
            "...       ...    ...    ...   ...    ...    ...    ...  ...   ...   ...  ...   \n",
            "317711    NaN    NaN    NaN   NaN    NaN    NaN    NaN   13   NaN   NaN   -7   \n",
            "317712    NaN    NaN    NaN   NaN    NaN    NaN    NaN   16   NaN   NaN   -7   \n",
            "317713    NaN    NaN    NaN   NaN    NaN    NaN    NaN   12   NaN   NaN   15   \n",
            "317714    NaN    NaN    NaN   NaN    NaN    NaN    NaN   18   NaN   NaN  -11   \n",
            "317715    NaN    NaN    NaN   NaN    NaN    NaN    NaN    0   NaN   NaN    0   \n",
            "\n",
            "          25     99    39     16     135  \n",
            "0       624.0  736.0 -58.0  918.0  119.0  \n",
            "1       631.0  723.0 -57.0  666.0  113.0  \n",
            "2       701.0  710.0 -58.0  659.0  107.0  \n",
            "3       710.0  695.0 -53.0  673.0  107.0  \n",
            "4       645.0  678.0 -76.0  673.0  108.0  \n",
            "...       ...    ...   ...    ...    ...  \n",
            "317711    NaN    NaN   NaN    NaN    NaN  \n",
            "317712    NaN    NaN   NaN    NaN    NaN  \n",
            "317713    NaN    NaN   NaN    NaN    NaN  \n",
            "317714    NaN    NaN   NaN    NaN    NaN  \n",
            "317715    NaN    NaN   NaN    NaN    NaN  \n",
            "\n",
            "[317716 rows x 16 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimeting with different imputation methods"
      ],
      "metadata": {
        "id": "nCE03FaqiiFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN impute for missing values\n",
        "\n",
        "# experiment with different values of k\n",
        "\n",
        "# try differerent imputation approaches here see what gets high accuracy for holdout later\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "\n",
        "\n",
        "# Create a KNN imputer with k=3 (you can adjust the value of k)\n",
        "#knn_imputer = KNNImputer(n_neighbors=4)\n",
        "\n",
        "# Perform imputation\n",
        "#df_train_imputed = pd.DataFrame(knn_imputer.fit_transform(df_train), columns=df_train.columns)\n",
        "\n",
        "#df_holdout = pd.DataFrame(knn_imputer.fit_transform(df_test), columns=df_test.columns)\n",
        "\n",
        "\n",
        "df_train_imputed = df_train.fillna(df_train.mean())\n",
        "df_holdout = df_test.fillna(df_test.mean())\n",
        "\n",
        "# df_train_imputed = df_train.fillna(df_train.median())\n",
        "# df_holdout = df_test.fillna(df_test.median())\n"
      ],
      "metadata": {
        "id": "Kj5al2SIinUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_imputed.to_csv('train_imputedmean.csv')\n",
        "df_holdout.to_csv('holdoutmean.csv')"
      ],
      "metadata": {
        "id": "CRgQjfp-ka4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}